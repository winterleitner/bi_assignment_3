---
output:
  bookdown::pdf_book:
    template: sample-sigchi.tex
    keep_tex: true
    citation_package: natbib
    fig_caption: true
title: Predicting the resale value of used cars
abstract: This is Assignment 3 in Business Intelligence @ TU Wien in the winter term of 2020.
bibliography: my-bibliography.bib

---

```{r setup, include=FALSE, message=FALSE}
library(tidyverse)
library(ggplot2)
library(GGally)
library(ggcorrplot)
library(mgcv)

audi <- read_csv("data/audi.csv")
audi['brand'] = "Audi"

bmw <- read_csv("data/bmw.csv")
bmw['brand'] = "BMW"

ford <- read_csv("data/ford.csv")
ford['brand'] = "Ford"

hyundi <- read_csv("data/hyundi.csv")
hyundi['brand'] = "Hyundai"

merc <- read_csv("data/merc.csv")
merc['brand'] = "Mercedes"

skoda <- read_csv("data/skoda.csv")
skoda['brand'] = "Skoda"

toyota <- read_csv("data/toyota.csv")
toyota['brand'] = "Toyota"

vauxhall <- read_csv("data/vauxhall.csv")
vauxhall['brand'] = "Vauxhall"

vw <- read_csv("data/vw.csv")
vw['brand'] = "VW"

car_data <- rbind(audi, bmw, ford, hyundi, merc, skoda, toyota, vauxhall, vw)
car_data <- car_data %>%
  select('brand', everything())
car_data <- car_data %>% relocate('price', .after = last_col())

#replace year with age
car_data$age = 2020 - car_data$year
```

# Introduction
For this project, we used a dataset found on Kaggle.com provided by user Aditya [@Aditya]. It contains a collection of different used car listings obtained by searching through online marketplaces using a web scraper. The dataset is split into different files, one per car brand. The brands for which data is available are:

* Audi
* BMW
* Ford
* Hyundai
* Mercedes
* Skoda
* Toyota
* Vauxhall (= Opel in Great Britain)
* VW

Additionally, the data set contains files with premade subsets of above mentioned car brands, for example *cclass.csv*, which contains only listings for the Mercedes model C Class. We chose to only utilize the unfiltered datasets.

Apart from the car brand, there are a number of other attributes available for each data entry.

* car model
* year of first registration
* transmission type
* mileage
* fuel type
* tax
* miles per gallon of fuel
* engine size

as well as the target variable price.

# Business Understanding

## a. Scenario

A group of entrepreneurs in the used car business want to counteract the ongoing trend of people selling their cars to other private individuals directly without involving commercial reseller, which has become very easy given the availability of online market places for used goods. The idea is the following: Customers are offered a new web-based platform where they can enter the most important key facts about the car they would like to sell. The platform immediately returns a first estimate of the price the platform owners would pay for the car. This estimate should be based on a model created from the used car listing data available.

## b. Business Objectives

The business objectives is in short:

**What is the expected value of a used car based on the given data entries?**

Answering this question helps the platform in multiple ways.

* make it more convenient for customers to sell their used car by getting an accurate first estimate right after entering the data
* increase revenues by missing out on fewer chances to buy used cars (more cars resold via the platform instead of directly to other buyers)
* speed up final evaluation of car value by offering a good starting point
* base offers made to customers on true market values

## c. Business Success Criteria

The following criteria need to be met by the prediction:

* The estimate should lead to a conversion rate of more than 30%, meaning that at least 30% of the users that enter their car data on the website actually proceed to sell their car on the platform.

* The estimations should never lead to an effective loss for the company. Therefore, estimations that are too high need to be avoided.


## d. Data Mining Goals

In order to fulfill the business objective of determining an accurate price estimate, a regression problem needs to be solved. The input data consists of the 9 attributes mentioned above.

## e. Data Mining Success Criteria

Regarding the result of the estimation, one important success criterion is:

* The estimate needs to be within a range of the actual price +/- 15% for 95% of the estimations made.

This is important because estimates that are further off the actual price may lead to:

* People aborting the process when the estimate is much lower than their expectation
* People entering the negotiations with far inflated expectation, effectively reducing the changes of the platform owners to score a good deal


# Data Understanding
In the following section, a data description report containing data types, statistical properties, data quality aspects as well as a visual exploration of data properties is presented.

## a. Data Types
The attributes in the data set have the data types shown in **Table \@ref(tab:table-datatypes).**

```{r echo=FALSE}
Attribute <- c("model", "age", "year", "transmission", "mileage", "fuelType", "tax", "mpg", "engineSize", "price");
Type <- c("String, nominal", "Integer, interval", "Integer, ratio", "String, nominal", "Integer, ratio", "String, nominal", "Integer, ratio", "Float, ratio", "Float, ratio", "Integer, ratio")
data_types <- data.frame(Attribute, Type)
```

```{r table-datatypes, echo=FALSE}
data_types %>%
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Data Types of the source data")
```


## b. Statistical Properties

```{r}
options(width = 60)
summary(car_data)
```


## c. Data Quality aspects
Since the data is recorded from the internet there is the possibility of it containing invalid information or missing values.

To begin with, we check the data for missing values. However, in this speficic case, there are none.
```{r}
dim(car_data) ==
  dim(car_data[complete.cases(car_data),])
```

Next up, we check plausibility of some of the extreme cases of numerical values. To keep it short, we only included one exemplary output her and then summarize the findings.
```{r}
options(width = 60)
head(car_data[order(car_data$age),], 5)
# year 2060 is an error

head(car_data[order(-car_data$age),], 5)
# the oldest cars seem realistic
```
```{r echo=FALSE, include=FALSE}
#MILEAGE
head(car_data[order(car_data$mileage),], 10)
# year 2008 and only one mile does not seem realistic, especially given the price.

head(car_data[order(-car_data$mileage),], 10)
# other end looks good

# TAX
head(car_data[order(car_data$tax),], 10)
# tax seems realistic on the low end ("slow" cars = low tax)

head(car_data[order(-car_data$tax),], 10)
# tax seems realistic on the top end (fast cars = high tax)

# MPG
head(car_data[order(car_data$mpg),], 10)
head(car_data[order(-car_data$mpg),], 10)
# for MPG, both high and low and values do not seem realistic. There is definitely some need for cleaning here.

# ENGINE SIZE
head(car_data[order(car_data$engineSize),], 10)
head(car_data[order(-car_data$engineSize),], 10)
# on the upper end, everything looks good, but there are entries with engine size = 0 for petrol and diesel cars, which is not possible

# Price
head(car_data[order(car_data$price),], 10)
head(car_data[order(-car_data$price),], 10)
# pricing for the extreme values looks pretty good, however in the top 10, we were wondering if the high prices for e.g. the Mercedes A-Class (>130.000) are authentic.
```
Most of the extreme values in the dataset were realistic. Some entries contain questionable combinations of age and mileage, unrealistically high or low MPG values or "0" engine sizes. In those cases, some filtering should be done.

## d. Visual Exploration of data properties and hypotheses

In the following figures, boxplots illustrate the ranges of the numeric (ratio) variables.

```{r figure-boxplots, echo=FALSE, out.width='0.98\\textwidth', fig.env='figure*', fig.cap="Boxplots on the distribution of the numeric attributes"}
par(mfcol = c(1, 5), mai=c(0.3,0.3,0.3,0.1), cex = 0.7)
boxplot(car_data$engineSize, main="Engine Size")
boxplot(car_data$mileage, main="Mileage")
boxplot(car_data$price, main="Price")
boxplot(car_data$mpg, main="MPG")
boxplot(car_data$tax, main="Tax")
par(mfcol = c(1, 1))
```
There are a few things we can learn from these diagrams. For example, it is interesting to see car listings contained in the data set are mostly for rather new cars, with a mileage median of less than 25000 miles. Taking a look at **Figure \@ref(fig:car-age)**, this suspicion is confirmed. The vast majority of cars in the dataset is indeed less than five years old. There is `r length(car_data$age[car_data$age < 0])` entry where the cars year is seemingly bigger than 2020, that is 2060, which will have to be dealt with in later steps.

```{r car-age, echo=FALSE, out.width='0.98\\columnwidth', fig.cap="Histogram of car age."}
hist(car_data$age[car_data$age >= 0], xlim= c(0,30), main="Histogram of car age", xlab="Age (cleaned age < 0 entries)")
```

The correlation plot in **Figure \@ref(fig:correlationmatrix)** shows some pretty good correlation between ther predictor variables and the price, so we might be able to create a solid regression using the data available.

```{r, correlationmatrix, echo=FALSE, out.width='0.98\\columnwidth', fig.cap="Pairs of all numeric attributes"}
ggcorrplot(cor(data.matrix(car_data)))
```
From the view point of a human estimating the value of a used car, the most influential attributes should be age, mileage and brand/model as well as general condition, which is however not part of our dataset. Looking at the correlation matrix, we see that there is indeed a significant correlation between price and age as well as mileage. For model and brand, the correlation is much lower.

# Data Preparation report


Insert content here.

a. Analyze options and potential for derived attributes (note: if the potential is considered low, these obviously do not necessarily have to be applied for your analysis, but options should be documented)
b. Analyze options for additional external data sources, attributes that might be useful to better address the business objectives or data mining goals (Note: this description may be hypothetical, i.e. you are not necessarily required to actually obtain and integrate the external data for the analysis)

#### e.g. Car Horse Power

c. Describe other pre-processing steps considered, specifying which ones were applied or not applied due to which reason. (e.g. data cleansing, transformations, binning, scaling, outlier removal, attribute removal, transcoding, ...) at a level of detail that ensures reproducibility of changes to the data. (Code may be supplied as supplement to the submission in case you produce your own code)


# Modeling

a. Identify suitable data mining algorithms and select one of these as the most suitable for your experiments, providing a brief justification.

The goal is to estimate the price for a car based on it inputed characteristics based on the training data containing nine attributes as well as the prediction lable. Therefore it is a supervised learning with regression, prediction of a numerical value.

When choosing the a regression model it has to deliver sufficient results, while being efficient in computation.
While complex regression models might deliver higher accuracies, they easily become difficult to interpret and follow back decisions.
Generalized linear regression is the got to regression model as it is fast to train and delivers good estimates and predictions. It's computational requirements depending on the data are managable.

b. Identify the hyper-parameters available for tuning in your chosen algorithm and select one that you deem most relevant for tuning, providing a brief justification.

Choosing a linear regression model leaves open the possibility to modify the formula to reproduce a linear relationship between the attributes and the predicted value. While doing it manually is very timeintensive and inefficient it can be implemented with the use of Generalized additive models (gam). This model allows to apply smoothing functions on the single parameters to improve prediction outcome. Effectively combining generalized linear models with additive models.


c. Define and document a train / validation / test set split, considering where necessary appropriate stratification, any dependencies between data instances (e.g. time series data) and relative sizes of the respective subsets.


Dividin the data train, validation and test set. In the ratio:
* train: 70%
* validate: 15%
* test: 15%

To ensure rproducability a seed is set.

```{r}
set.seed(1)
assignment <- sample(1:3,nrow(car_data),prob = c(0.7,0.15,0.15), replace = TRUE)

train_data <- car_data[assignment==1,]
test_data <- car_data[assignment==2,]
validate_data <- car_data[assignment==3,]
```

Splitting the data in train, test and validation samples, caused failuers in the prediciton process, as some car models are very scarse or unique. Meaning the case happened of certain models appearing only in the testing or validation samples and therefore being unknown levels to the model. To resolve this issue, new levels (car models not present in the training sample) were removed from the testing and validation sample.
Further thoughtprocess on this matter in regards of usability in business are present in **ADD REFERENCE HERE!**

```{r out.width='0.98\\columnwidth'}
# Analysing data on the carmodels
head(sort(table(car_data$model),decreasing = TRUE),5)
tail(sort(table(car_data$model),decreasing = TRUE),5)
# reveals that some carmodels are only present once in the datamodel

# Carmodels missing in the trainingdata
missing_fact_test <- setdiff(unique(test_data$model), unique(train_data$model))
missing_fact_validate <- setdiff(unique(validate_data$model), unique(train_data$model))

cleaned_test_data <- test_data[!test_data$model %in% missing_fact_test,]
cleaned_validate_data <- validate_data[!validate_data$model %in% missing_fact_validate,]
```

d. Train the model on the training set and comparing the performance on the validation set to identify the best hyper-parameter setting, explicitly documenting all parameter settings (avoid stating simply to have used “default parameters”, focus on reproducibility of the results you report).

```{r, echo=FALSE, include=FALSE}

formula <- price~brand+mpg+mileage+year+transmission+fuelType+tax+engineSize
formula2 <- price~brand+mpg+mileage+year+transmission+fuelType+tax+engineSize+model

# encountered issue -> depending on the training and testing set it can be that one model is newly introduced in the testset which is not presen in the training set.
# This could also happen in the daily use of the system when new carmodels are introduces. How can this be handeld?
# Looking at the correlation matrix, the model of a car does not have high correlation with the price



# glm
glm_model <- lm(formula2,data = train_data)
glm_pred <- predict(glm_model,cleaned_test_data)

# decision tree
# library(rpart)
# tree_model <- rpart(formula2,method = "anova", data = car_data[trainsample,])
# tree_pred <- predict(tree_model, cleaned_testing_data)

# Build the model
gam_model <- gam(price~ brand+s(mpg)+s(mileage)+s(year)+transmission+fuelType+tax+s(engineSize)+model, data = train_data)
# Make predictions
gam_pred <- gam_model %>% predict(cleaned_test_data)
```


e. Report suitable performance metrics supported, where possible, by figures/graphs showing the tuning process of the hyper parameter.



```{r}
# summary
summary(glm_model)$r.squared
summary(gam_model)$r.sq
```

```{r}

plot(cleaned_test_data$price,glm_pred)
plot(cleaned_test_data$price,gam_pred)

hist(cleaned_test_data$price-glm_pred,breaks = 200)
hist(cleaned_test_data$price-gam_pred,breaks = 200)

mean(abs((cleaned_test_data$price - glm_pred)/cleaned_test_data$price))
boxplot(cleaned_test_data$price - glm_pred)

# mean(abs(cleaned_testing_data$price - tree_pred))
# hist(abs(cleaned_testing_data$price - tree_pred),200)

mean(abs((cleaned_test_data$price - gam_pred)/cleaned_test_data$price))
boxplot(cleaned_test_data$price - gam_pred)

summary(abs((cleaned_test_data$price - gam_pred)/cleaned_test_data$price))
```

# Evaluation
Insert content here.

a. Apply the final model on the test data and document performance.

```{r}
# Evaluate models
gam_pred_val <- gam_model %>% predict(cleaned_validate_data)

mean(abs((cleaned_validate_data$price - gam_pred_val)/cleaned_validate_data$price))
boxplot(cleaned_validate_data$price - gam_pred_val)

summary(abs((cleaned_validate_data$price - gam_pred_val)/cleaned_validate_data$price))
```

b. Re-train the model with identical hyper-parameters using the full train and validation data and again apply it on the test data, documenting the performance
c. Identify and document
    i. state-of-the-art performance from the literature using the same (albeit potentially slightly differently pre-processed) data set from the literature.
    ii. the expected base-line performance of a trivial acceptor / rejecter or random classifier
d. Compare the performance achieved with the benchmark and baseline performances (c.f. Section 1e – Data Mining success Criteria) according to different metrics (i.e. overall, but also on per-class level (confusion matrix), micro/macro precision/recall in the case of classification tasks, regression errors in certain parts of the data space, ... (Note your goal is not necessarily to obtain a better result than what has been reported in the state of the art, this is not a grading criterion! On the other hand, if the performance of your classifiers is massively below the stat of the art (or even below a random baseline or trivial acceptor / rejecter) you may want to investigate the reason...)
e. Compare the performance obtained with the Data Mining success criteria defined in the business understanding phase.


# Deployment
Insert content here.

a. Compare the performance obtained with respect to the needs for addressing the business success criteria and provide recommendations for deployment (fully automatic, hybrid solutions, deploying only for a part of the data space, ...) as well as recommendations for subsequent analysis.
b. Consider and briefly document potential ethical aspects as well as impact assessment / risks identified in deployment
c. Document aspects to be monitored during deployment, specifying triggers that should lead to intervention.
d. Briefly re-visit reproducibility aspects reflecting on aspects well documented and those that might pose a risk in terms of reproducibility based solely on the information provided in this report


# Summary of findings
Insert content here.


a. Briefly summarize your overall findings and lessons learned
b. (optional) Provide feedback on this exercise in general: which parts were useful / less useful; which other kind of experiment would have been interesting, ... (this section is, obviously, optional and will not be considered for grading. You may also decide to provide that kind of feedback anonymously via the feedback mechanism in TISS – in any case we would appreciate learning about it to adjust the exercises for next year following a major re-structuring this year based on feedback obtained.)
