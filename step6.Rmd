---
output: 
  bookdown::pdf_book:
    template: sample-sigchi.tex
    keep_tex: true
    citation_package: natbib
    fig_caption: true
title: Predicting the resale value of used cars
abstract: This is Assignment 3 in Business Intelligence @ TU Wien in the winter term of 2020.
bibliography: my-bibliography.bib

---

```{r setup, include=FALSE, message=FALSE}
library(tidyverse)
library(ggplot2)

audi <- read_csv("data/audi.csv")
audi['brand'] = "Audi"

bmw <- read_csv("data/bmw.csv")
bmw['brand'] = "BMW"

ford <- read_csv("data/ford.csv")
ford['brand'] = "Ford"

hyundi <- read_csv("data/hyundi.csv")
hyundi['brand'] = "Hyundai"

merc <- read_csv("data/merc.csv")
merc['brand'] = "Mercedes"

skoda <- read_csv("data/skoda.csv")
skoda['brand'] = "Skoda"

toyota <- read_csv("data/toyota.csv")
toyota['brand'] = "Toyota"

vauxhall <- read_csv("data/vauxhall.csv")
vauxhall['brand'] = "Vauxhall"

vw <- read_csv("data/vw.csv")
vw['brand'] = "VW"

car_data <- rbind(audi, bmw, ford, hyundi, merc, skoda, toyota, vauxhall, vw)
```

# Introduction
For this project, we used a dataset found on Kaggle.com provided by user Aditya [@Aditya]. It contains a collection of different used car listings obtained by searching through online marketplaces using a web scraper. The dataset is split into different files, one per car brand. The brands for which data is available are:

* Audi
* BMW
* Ford
* Hyundai
* Mercedes
* Skoda
* Toyota
* Vauxhall (= Opel in Great Britain)
* VW

Additionally, the data set contains files with premade subsets of above mentioned car brands, for example *coclass.csv*, which contains only listings for the Mercedes model C Class. We chose to only utilize the unfiltered datasets.

Apart from the car brand, there are a number of other attributes available for each data entry.

* car model
* year of first registration
* transmission type
* mileage
* fuel type
* tax
* miles per gallon of fuel
* engine size

as well as the target variable price.

# Business Understanding

## a. Scenario

A group of entrepreneurs in the used car business want to counteract the ongoing trend of people selling their cars to other private individuals directly without involving commercial reseller, which has become very easy given the availability of online market places for used goods. The idea is the following: Customers are offered a new web-based platform where they can enter the most important key fact about the car they would like to sell. The platform the immediately returns a first estimate of the price the platform owners would pay for the car. This estimate should be based on a model created from the used car listing data available.


## b. Business Objectives

The business objectives is iin short:

**What is the expected value of a used car based on the given data entries?**

Answering this question helps the platform in multiple ways.

* make it more convenient for customers to sell their used car but getting an accurate first estimate right after entering the data
* increase revenues by missing out on fewer chances to buy used cars (more cars resold via the platform instead of directly to other buyers)
* speed up final evaluation of car value by offering a good starting point
* base offers made to customers on true market values

## c. Business Success Criteria

The following criteria need to be met by the prediction:

* The estimate should lead to a conversion rate of more than 30%, meaning that at least 30% of the users that enter their car data on the website actually proceed to sell their car on the platform.

* The estimations should never lead to an effective loss for the company. Therefore, estimations that are too high need to be avoided.


## d. Data Mining Goals

In order to fulfill the business objective of determining an accurate price estimate, a regression problem needs to be solved. The input data consists of the 9 attributes mentioned above. 

## e. Data Mining Success Criteria

Regarding the result of the estimation, one important success criterion is:

* The estimate needs to be within a range of the actual price +/- 10% for 95% of the estimations made.

This is important because estimates that are further off the actual price may lead to:

* People aborting the process when the estimate is much lower than their expectation
* People entering the negotiations with far inflated expectation, effectively reducing the changes of the platform owners to score a good deal


Citation example.[@Aditya]
Plot example.**The maturation of the song over time is shown in Figure \@ref(fig:tribute-plot).**

```{r tribute-plot, echo=FALSE, out.width='0.98\\columnwidth', fig.cap="This is how great Tribute gets over time"}
plot(pressure)
```

Small plot example.
```{r two-col-tribute-plot, echo=FALSE, out.width='0.98\\textwidth', fig.cap="This is a two-column plot of how great Tribute gets over time", fig.env='figure*'}
plot(pressure)
```


# Data Understanding
In the following section, a data description report containing data types, statistical properties, data quality aspects as well as a visual exploration of data properties is presented.

## a. Data Types
The attributes in the data set have the data types shown in **Table \@ref(tab:table-datatypes).**

```{r echo=FALSE}
Attribute <- c("model", "year", "transmission", "mileage", "fuelType", "tax", "mpg", "engineSize", "price");
Type <- c("String, nominal", "Integer, interval", "String, nominal", "Integer, ratio", "String, nominal", "Integer, ratio", "Float, ratio", "Float, ratio", "Integer, ratio")
data_types <- data.frame(Attribute, Type)
```

```{r table-datatypes, echo=FALSE}
data_types %>%
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Data Types of the source data")
```


## b. Statistical Properties

```{r echo=FALSE}

car_data
```

Data Description Report presenting
a. Data types,
b. Statistical properties
c. Data Quality aspects
d. Visual Exploration of data properties and hypotheses

## d. Visual Exploration

In the following figures, boxplots illustrate the ranges of the numeric (ratio) variables.

```{r figure-boxplots, echo=FALSE, out.width='0.98\\columnwidth', fig.cap="This is how great Tribute gets over time"}
par(mfcol = c(1, 5), mai=c(0.3,0.3,0.3,0.1), cex = 0.7)
boxplot(car_data$engineSize, main="Engine Size")
boxplot(car_data$mileage, main="Mileage")
boxplot(car_data$price, main="Price")
boxplot(car_data$mpg, main="MPG")
boxplot(car_data$tax, main="Tax")
```


Table example.

```{r table-iris, echo=FALSE}
iris %>%
  select(-Species) %>%
  head(10) %>%
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Example Table Showcasing capabilities")
```



# Data Preparation report
Insert content here.

a. Analyze options and potential for derived attributes (note: if the potential is considered low, these obviously do not necessarily have to be applied for your analysis, but options should be documented)
b. Analyze options for additional external data sources, attributes that might be useful to better address the business objectives or data mining goals (Note: this description may be hypothetical, i.e. you are not necessarily required to actually obtain and integrate the external data for the analysis)

#### e.g. Car Horse Power

c. Describe other pre-processing steps considered, specifying which ones were applied or not applied due to which reason. (e.g. data cleansing, transformations, binning, scaling, outlier removal, attribute removal, transcoding, ...) at a level of detail that ensures reproducibility of changes to the data. (Code may be supplied as supplement to the submission in case you produce your own code)


# Modeling
Insert content here.

a. Identify suitable data mining algorithms and select one of these as the most suitable for your experiments, providing a brief justification.
b. Identify the hyper-parameters available for tuning in your chosen algorithm and select one that you deem most relevant for tuning, providing a brief justification.
c. Define and document a train / validation / test set split, considering where necessary appropriate stratification, any dependencies between data instances (e.g. time series data) and relative sizes of the respective subsets.
d. Train the model on the training set and comparing the performance on the validation set to identify the best hyper-parameter setting, explicitly documenting all parameter settings (avoid stating simply to have used “default parameters”, focus on reproducibility of the results you report).
188.429 Business Intelligence (VU 4,0) – WS 2020/21 Assignment 2: Data Analytics
e. Report suitable performance metrics supported, where possible, by figures/graphs showing the tuning process of the hyper parameter.


# Evaluation
Insert content here.

a. Apply the final model on the test data and document performance.
b. Re-train the model with identical hyper-parameters using the full train and validation data and again apply it on the test data, documenting the performance
c. Identify and document
    i. state-of-the-art performance from the literature using the same (albeit potentially slightly differently pre-processed) data set from the literature.
    ii. the expected base-line performance of a trivial acceptor / rejecter or random classifier
d. Compare the performance achieved with the benchmark and baseline performances (c.f. Section 1e – Data Mining success Criteria) according to different metrics (i.e. overall, but also on per-class level (confusion matrix), micro/macro precision/recall in the case of classification tasks, regression errors in certain parts of the data space, ... (Note your goal is not necessarily to obtain a better result than what has been reported in the state of the art, this is not a grading criterion! On the other hand, if the performance of your classifiers is massively below the stat of the art (or even below a random baseline or trivial acceptor / rejecter) you may want to investigate the reason...)
e. Compare the performance obtained with the Data Mining success criteria defined in the business understanding phase.


# Deployment
Insert content here.

a. Compare the performance obtained with respect to the needs for addressing the business success criteria and provide recommendations for deployment (fully automatic, hybrid solutions, deploying only for a part of the data space, ...) as well as recommendations for subsequent analysis.
b. Consider and briefly document potential ethical aspects as well as impact assessment / risks identified in deployment
c. Document aspects to be monitored during deployment, specifying triggers that should lead to intervention.
d. Briefly re-visit reproducibility aspects reflecting on aspects well documented and those that might pose a risk in terms of reproducibility based solely on the information provided in this report


# Summary of findings
Insert content here.


a. Briefly summarize your overall findings and lessons learned
b. (optional) Provide feedback on this exercise in general: which parts were useful / less useful; which other kind of experiment would have been interesting, ... (this section is, obviously, optional and will not be considered for grading. You may also decide to provide that kind of feedback anonymously via the feedback mechanism in TISS – in any case we would appreciate learning about it to adjust the exercises for next year following a major re-structuring this year based on feedback obtained.)